---
title: "Ethics and Practice in Data Science Final Project"
author: "International Commuters"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

#Overview & Goals

This markdown file will be the place where we can conduct our exploratory data analysis.

Below you will find the questions that we are using to explore the data and who is
responsible for conducting the data analysis associated with that question.

By the end of this process we will have as a team:

1. Imported the cleaned dataset from Lab 3

2. All of the EDA questions examined with clear code & written explanations of the results.

3. Data visualization for the various EDAs.
    - One question to resolve is - is Lilly creating all the viz or should this be the responsibility
      the people assigned the questions.
    - Either way we need one coherent theme across all viz to be imported into the presentation

4. A coherent final presentation (in a separate powerpoint created by Lilly), along with a clear
   assignment of who is presenting what.

LINK TO SLIDES: https://docs.google.com/presentation/d/1Z7EedURd1mhE6WoUVpifnr4v6w-2GmTj2a6IKPiLOHg/edit?usp=sharing

*General notes:
>Some example code for cleaning and data analysis will be pulled into the presentation,
so make sure you are annotating code as you go so we can explain what each line does if needed.

>keep committing your progress with clear messages so we can see contributions and troubleshoot
errors if needed.

#Task Assignment

### **1. Traffic & Volume Patterns**
>Lilly is going to be working on this question

* **How does bidding volume change across hours of the day and days of the week?**
* **When are the peak bidding periods, and what might explain those spikes?**

---

### **2. Bid Behavior & Competitiveness**
>Edwin is going to be working on this question

* **How do bid amounts differ between winning and losing bids?**
* **Is there a relationship between bid amount and the probability of winning an auction?**
* **Do certain advertisers consistently place higher or lower bids compared to others?**

---

### **3. Auction Outcomes**
>Bingtang is going to be working on this question

* **Which features (e.g., bid amount, time of day, advertiser) are most predictive of a win?**
* **Are there systematic differences in outcomes across devices, regions, or ad categories?**

---

### **4. Data Quality–Driven Questions**
>Neal is going to be working on this question

* **How do missing or inconsistent fields (e.g., timestamps, IDs, bid amounts) influence auction outcomes or patterns?**
* **How does the cleaned dataset change our interpretation of key trends compared to the raw data?**

---

#Setup

##Packages
```{r loadpackages}

library(tidyverse)
library(arrow)
library(logger)
library(glue)
library(dplyr)
library(tidyr)
library(rlang)
library(lubridate)
library(tictoc)
library(here)
library(jsonlite)
library(scales)
library(knitr)
library(kableExtra)
library(DT)
library(tigris)   # for zctas()
library(sf)       # for spatial functions
library(stringr)  # for string cleaning
# source(here("src", "data_cleaning.r"))
library(zipcodeR)
library(arrow)
library(ggridges)   # ridge plots
library(tigris)     # US geographic boundaries
library(sf)         # spatial data handling
library(viridis)    # colorblind-safe palettes(maps)
library(mgcv)       # GAM/BAM modeling
library(pROC)

source(here("src", "data_cleaning.r"))  # Functions loaded at runtime - linter warnings are false positives


#Lilly imported these from Lab 3, please feel free to add any additional libraries we might need.

```

*Note: please try to use tidyverse packages and functions, we want to make sure everyone is familiar
with the functions we are using.

#Import the cleaned dataset

```{r}
#READ THIS BEFORE RUNNING CODE
#Since we cannot "save" the cleaned data in lab 03 please make sure to run this code "saveRDS(bids_clean, here::here("data", "bids_clean.rds"))" at line 1318 and make sure to NOT commit that change, then you can run this code


# bids_clean <- readRDS(here::here("data", "bids_clean.rds"))


data_cleaning_pipeline <- function(df, expected_columns, zip_code_db = NULL, save_path = NULL, verbose = TRUE) {
  df <- clean_price_column(df,
                           min_price = 0,
                           max_price = 10,
                           fix_leading_o = TRUE,
                           verbose = verbose)

  df <- clean_geo_region_column(df,
                                verbose = verbose)

  df <- clean_zip_column(df,
                         zip_code_db = zip_code_db,
                         verbose = verbose)

  df <- clean_response_time_column(df,
                                   col_name = "RESPONSE_TIME",
                                   output_col_name = "RESPONSE_TIME_clean",
                                   extract_digits = TRUE,
                                   verbose = verbose)

  df <- clean_timestamp_column(df,
                               col_name = "TIMESTAMP",
                               verbose = verbose)

  df <- clean_city_column(df,
                          zip_code_db = zip_code_db,
                          verbose = verbose)

  df <- clean_geo_coordinates_column(df,
                                     verbose = verbose)

  df <- clean_bids_won_column(df,
                              verbose = verbose)

  df <- clean_date_column(df,
                          col_name = "DATE_UTC",
                          output_col_name = "DATE_UTC_clean",
                          verbose = verbose)

  df <- clean_device_type_column(df,
                                 col_name = "DEVICE_TYPE",
                                 output_col_name = "DEVICE_TYPE_clean",
                                 verbose = verbose)

  df <- clean_response_time_column(df,
                                   col_name = "RESPONSE_TIME",
                                   output_col_name = "RESPONSE_TIME_clean",
                                   extract_digits = TRUE,
                                   verbose = verbose)

  df <- clean_requested_sizes_column(df,
                                     col_name = "REQUESTED_SIZES",
                                     output_col_name = "REQUESTED_SIZES_clean",
                                     verbose = verbose)

  duplicate_handler <- remove_duplicates(df,
                                         exclude_cols = c("row_id"),
                                         verbose = verbose)
  df <- duplicate_handler[["df"]]
  removed_indices <- duplicate_handler[["removed_indices"]]

  if (!is.null(save_path)) {
    write_parquet(df, save_path)
  }

  return(df)
}


# ---- Timing Start ----
run_time <- system.time({

#------------------------------------------------------
# LOAD EXPECTED BIDS COLUMNS FROM CSV
#------------------------------------------------------
expected_columns <- data.frame(readr::read_csv(
  here::here("src", "expected_columns.csv"),
  col_types = "ccc"
))

#------------------------------------------------------
# LOAD BIDS DATA FROM PARQUET
#------------------------------------------------------
cat("\n")
cat(strrep("=", 70), "\n")
cat("STARTING BIDS DATA PROCESSING\n")
cat(strrep("=", 70), "\n\n")

# Load data
cat("Loading data...\n")
original_bids <- read_parquet(here("data", "bids_data_vDTR.parquet"))
bids <- original_bids %>% mutate(row_id = row_number())
cat(glue("Loaded {nrow(original_bids)} rows and {ncol(original_bids)} columns\n\n"))

#------------------------------------------------------
# LOAD ZIPCODE DATA
#------------------------------------------------------
# Load ZIP → city lookup from zipcodeR
zip_code_db <- load_oregon_zips()

#------------------------------------------------------
# CHECK FOR MISSING COLUMNS
#------------------------------------------------------
missing_columns <- check_columns(bids, expected_columns$column)
cat(glue::glue("There are {length(missing_columns)} missing column(s): \n {paste(missing_columns, collapse = ', ')}"))

#------------------------------------------------------
# TYPE SUMMARY
#------------------------------------------------------
bids_type_summary <- check_column_types(bids, expected_columns)
print(bids_type_summary)

save_path <- NULL
# save_path <- here("data", "bids_data_vDTR_clean.parquet")
bids <- data_cleaning_pipeline(bids, expected_columns, zip_code_db, save_path, verbose = TRUE)

cat("\n")
cat(strrep("=", 70), "\n")
cat("CREATING FINAL CLEANED DATASET\n")
cat(strrep("=", 70), "\n\n")
# Create final cleaned dataset
bids_clean <- bids %>%
  select(
    row_id,
    DATE_UTC_clean,
    TIMESTAMP_clean,
    AUCTION_ID,
    PUBLISHER_ID,
    PRICE_final,
    DEVICE_GEO_REGION_clean,
    DEVICE_GEO_ZIP_clean,
    DEVICE_GEO_CITY_clean,
    DEVICE_GEO_LAT_clean,
    DEVICE_GEO_LONG_clean,
    BID_WON_clean,
    RESPONSE_TIME_clean,
    DEVICE_TYPE_clean,
    SIZE,
    REQUESTED_SIZES_clean
  )
  # %>%
  # rename_with(~ str_remove(., "(_clean|_final)$"))


print(class(bids_clean))
glimpse(bids_clean)
# NA counts per column
na_count_by_col <- colSums(is.na(bids_clean %>% select(-REQUESTED_SIZES_clean)))
cat("\nNA Counts per Column:\n")
cat(strrep("=", 70), "\n")
print(na_count_by_col)
total_na_rows <- sum(!complete.cases(bids_clean %>% select(-REQUESTED_SIZES_clean)))
print(glue("Total NA rows: {total_na_rows}"))

})  # ---- Timing End ----

cat(glue::glue("\n\nTotal runtime for data cleaning: {round(run_time[['elapsed']], 2)} seconds\n"))


```
```{r}
glimpse(bids_clean)
```

#Run Exploratory Data Analysis

### **1. Traffic & Volume Patterns**

***How does bidding volume change across hours of the day and days of the week?**

```{r}
# Extract hour and day of week
bids_clean <- bids_clean %>%
  mutate(
    hour = hour(TIMESTAMP_clean),
    day_of_week = wday(TIMESTAMP_clean, label = TRUE, abbr = FALSE, week_start = 1),

    day_of_week = factor(day_of_week,
                         levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
                         ordered = FALSE)
  )

# Summarize bid volume by hour
hourly_volume <- bids_clean %>%
  count(hour)


all_days <- factor(
  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
  levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
  ordered = FALSE
)

# Summarize bid volume by day of week (include 0s)
daily_volume <- bids_clean %>%
  count(day_of_week) %>%
  complete(day_of_week = all_days, fill = list(n = 0))

# Plot: Bidding volume by hour
ggplot(hourly_volume, aes(x = hour, y = n, fill = as.factor(hour))) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(
    breaks = 0:23,
    labels = format(strptime(0:23, format = "%H"), format = "%I %p")
  ) +
  scale_fill_viridis_d(option = "cividis") +
  labs(
    title = "Bidding Volume by Hour of Day",
    x = "",
    y = "# of Bids"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot: Bidding volume by day of week
ggplot(daily_volume, aes(x = day_of_week, y = n, fill = day_of_week)) +
  geom_col(show.legend = FALSE) +
  scale_y_continuous(labels = label_comma()) +
  scale_fill_viridis_d(option = "cividis") +
  labs(
    title = "Bidding Volume by Day of Week",
    x = "",
    y = "# of Bids"
  ) +
  theme_minimal()


```

Comments: At first I thought there was something wrong since we are only seeing results from a Tuesday and Wednesday, but after investigation, the original data set given to use only has data from 10/21/2025 and 10/22/2025, so this makes sense. This will limit the conclusions we can draw from the data, but it is not error.

Note: For the plots I intentionally added a minimal theme and made sure that the color schemes added were color-blind friendly and accessible. Specifically "cividis" is most optimized for all types of vision, including grayscale.

***When are the peak bidding periods, and what might explain those spikes?**

```{r}

# Identify top 3 bidding hours
top_hours <- hourly_volume %>%
  top_n(3, n) %>%
  arrange(desc(n))

# Identify peak day(s)
top_days <- daily_volume %>%
  top_n(1, n)

print(top_hours)
print(top_days)

```

The top three peak bidding hours are 2am, 3am, and 1am with 67,111 bids, 61,514 bids, and 60,233 bids respectively. These early hour spikes suggest that the bidding system might be automated or operating across multiple timezones (if bidders are from outside of the PST), but that is not the case here as all the bids come from OR. Another explanation could be backlogged jobs. Since it is very unlikely that these bids come from humans at these early/late hours it points to scheduled systems or ad bots.

Wednesday had the highest volume with 273,779 bids, but again we only have data from two days of the week so that does not mean much here.

### **2. Bid Behavior & Competitiveness**

## Price difference between winning and losing bids

```{r}
# Ridgeline plot of price distributions for won vs lost bids

ggplot(bids_clean, aes(y = BID_WON_clean, x = PRICE_final, fill = BID_WON_clean)) +
  geom_density_ridges(alpha = 0.7, scale = 1.2) +      # ridge density curves
  scale_fill_manual(
    values = c("FALSE" = "#D55E00", "TRUE" = "#009E73"), # custom colors
    labels = c("Lost Bid", "Won Bid")                    # legend labels
  ) +
  labs(
    title = "Bid Price Distribution by Bid Outcome",      # plot title
    subtitle = "Ridgeline plot highlights distribution shapes and overlap",
    x = "Bid Price",                                      # x-axis label
    y = "Bid Outcome"                                     # y-axis label
  ) +
  scale_y_discrete(labels = c("FALSE" = "Lost Bids", "TRUE" = "Won Bids")) +
  theme_minimal(base_size = 13) +                         # clean theme
  theme(plot.title = element_text(face = "bold"))         # bold title


```

### Figure summary

The ridgeline plot gives a clear picture of how bid prices differ between winning and losing bids. Most losing bids are packed tightly at the lower end of the price range, while winning bids tend to extend into slightly higher prices. There is some overlap at very low bid amounts, but overall the curve for winning bids stretches farther to the right. This pattern suggests that higher bid prices generally improve the chances of winning.

## Violin - Boxplot bid outcome representation

```{r}
# Violin + Boxplot comparing bid prices for wins vs losses

ggplot(bids_clean, aes(x = BID_WON_clean, y = PRICE_final, fill = BID_WON_clean)) +
  geom_violin(trim = FALSE, alpha = 0.6) +                 # violin shows price distribution shape
  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.9) +  # boxplot shows median + IQR
  scale_fill_manual(
    values = c("FALSE" = "#D55E00", "TRUE" = "#009E73"),   # custom colors
    labels = c("Lost Bid", "Won Bid"),                     # legend labels
    name = "Bid Outcome"
  ) +
  labs(
    title = "Distribution of Bid Prices by Bid Outcome",    # plot title
    subtitle = "Violin + Boxplot highlight price differences",
    x = "Bid Outcome",
    y = "Bid Price"
  ) +
  scale_x_discrete(labels = c("FALSE" = "Lost Bids", "TRUE" = "Won Bids")) +
  theme_minimal(base_size = 13) +                           # clean theme
  theme(
    plot.title = element_text(face = "bold", size = 15),    # bold title
    strip.text = element_text(face = "bold"),
    legend.position = "none"                                # hide legend (labels on x-axis)
  )

```

### Figure summary

The violin–boxplot shows clear differences in how bid prices behave for winning and losing bids. Losing bids are mostly concentrated at very low prices, with only a few stretching upward. In contrast, winning bids tend to be higher overall, with a wider spread and a noticeably higher median. While there is still some overlap at the lower end, the shape of the distributions shows that higher bid prices are much more common among winning bids. Overall, the plot suggests that bidding slightly more greatly improves the odds of winning.

## Publishers bidding trends

```{r}
# Boxplot of price distribution across advertisers

ggplot(bids_clean, aes(x = factor(PUBLISHER_ID), y = PRICE_final,
                       fill = factor(PUBLISHER_ID))) +
  geom_boxplot(show.legend = FALSE) +     # boxplots for each advertiser
  coord_flip() +                          # horizontal orientation for readability
  labs(
    title = "Bid Distributions by Publishers",
    x = "Publisher",
    y = "Bid Amount"
  )

```

### Figure summary

The boxplot highlights how bidding behavior varies widely across advertisers. Some advertisers place consistently low and tightly clustered bids, while others submit much higher bids with a broader spread. Several advertisers show long tails and many outliers, indicating occasional aggressive bidding. A few advertisers also stand out with significantly higher median bid values compared to the rest. Overall, the plot shows that bidding strategies differ substantially across advertisers, with some adopting conservative pricing and others frequently pushing toward the upper end of the bid range.

## Publishers spatial price representation

```{r}
# Hexbin map showing spatial bid patterns by advertiser

ggplot(bids_clean, aes(DEVICE_GEO_LONG_clean, DEVICE_GEO_LAT_clean)) +
  stat_summary_hex(aes(z = PRICE_final), fun = mean) +   # hexbin with mean price per hex
  scale_fill_viridis_c(trans = "log",                    # color scale on log scale for contrast
                       name = "Avg Bid Price (log scale)") +
  coord_equal() +                                        # preserve geographic proportions
  facet_wrap(~ PUBLISHER_ID) +                           # separate map panel per advertiser
  labs(
    title = "Spatial Patterns of Publisher Bid Prices Across Oregon",
    x = "Longitude",
    y = "Latitude"
  )

```

### Plot summary

The faceted map shows how public vary not only in the prices they bid but also in where those bids appear across Oregon. Some publishers have activity spread broadly throughout the state, while others are concentrated in just a few regions. Within each publisher’s panel, the color shading highlights differences in bid intensity: lighter areas represent higher average prices, while darker areas reflect lower prices. The patterns suggest that publishers target different geographic areas and may adjust their bidding strategies depending on where users are located. Thus the map reveals distinct spatial footprints for each publisher, with both bidding levels and geographic focus varying considerably from one publisher to another.

## Cleaning NAs for modeling

```{r}
# Clean binary column from the original character ones
bids_clean$BID_WON_clean <- ifelse(bids_clean$BID_WON_clean == "TRUE", 1,
                                   ifelse(bids_clean$BID_WON_clean == "FALSE", 0, NA))

# drop NAs
bids_clean <- bids_clean %>%
  filter(
    !is.na(BID_WON_clean),
    !is.na(PRICE_final),
    !is.na(DEVICE_GEO_LONG_clean),
    !is.na(DEVICE_GEO_LAT_clean)
  )
```

## predicting win probability from price and location

```{r}
set.seed(123)   # ensure the train/test split is reproducible

# Determine number of rows in the dataset
n <- nrow(bids_clean)

# Randomly select 60% of rows for training
train_index <- sample(seq_len(n), size = 0.6 * n)

# Split the dataset into training and testing sets
train_data <- bids_clean[train_index, ]   # 60% training data
test_data  <- bids_clean[-train_index, ]  # remaining 40% test data

# Fit a BAM model (fast GAM) to predict win probability
model <- bam(
  BID_WON_clean ~
    s(PRICE_final, k = 10) +                            # smooth effect of bid price
    te(DEVICE_GEO_LONG_clean, DEVICE_GEO_LAT_clean,     # 2D smooth for spatial effects
       k = c(8, 8)),
  data = train_data,                                     # training dataset
  family = binomial(),                                   # binary outcome (win or lose)
  discrete = TRUE                                        # speed optimization for big data
)

summary(model)   # show model fit statistics and significance

```

### Model Interpretation

The model shows a strong and clear relationship between how much an advertiser bids and their chances of winning the auction. The smooth term for bid amount (s(PRICE_final)) is highly significant (p < 2e-16), meaning changes in bid price have a meaningful effect on the probability of winning. In general, higher bids increase the likelihood of winning, which aligns with how auction systems typically function.

The spatial smooth term (s(longitude, latitude)) is also significant, indicating that location matters as well—some geographic areas are naturally more competitive than others.

The model explains about 30% of the deviance and has an adjusted R-squired of 0.345, which is reasonable for behavioral auction data. Overall, the results suggest that while bid amount is a major driver of winning probability, location-based competition also plays an important role.

## Model dicrimination accuracy

```{r}
# Predict on test data
test_data$pred_prob <- predict(model, newdata = test_data, type = "response")

# Compute AUC
roc_obj <- roc(test_data$BID_WON_clean, test_data$pred_prob)
auc_value <- auc(roc_obj)

print(auc_value)
```
### AUC Interpretation

The AUC of 0.86 shows that our model is very effective at separating winning and losing bids, about 86% of the time, it assigns a higher win probability to an actual winner than to a loser. This indicates strong predictive power, meaning the model is reliable for estimating competitive bid prices, identifying locations where bids tend to perform better or worse, and predicting the likelihood of winning for new bids.

## Trend to our predicted model

```{r}
# showing price in relation to probability of win or loss a bid

ggplot(test_data, aes(x = PRICE_final, y = pred_prob)) +
  geom_point(alpha = 0.1, color = "#009E73") +
  geom_smooth(color = "#D55E00", se = FALSE, size = 1.2) +
  labs(
    title = "Predicted Win Probability vs Bid Price",
    x = "Bid Price",
    y = "Predicted Win Probability"
  ) +
  theme_minimal(base_size = 13)

```

### Figure summary

The figure shows how win probability changes with bid price. Win chances rise quickly at low prices, meaning small increases make a big difference. As prices move higher, the improvement slows, and eventually the probability levels off, suggesting that once a bid is competitive, raising the price further adds little benefit.

## spatial representation of win probability across cities

```{r}
# Predict win probability using your trained model
bids_clean$pred_prob <- predict(
  model,
  newdata = bids_clean,
  type = "response"
)

# Prepare data for hex plotting
bids_pred_hex <- bids_clean %>%
  mutate(
    DEVICE_GEO_LONG_clean = as.numeric(DEVICE_GEO_LONG_clean),
    DEVICE_GEO_LAT_clean  = as.numeric(DEVICE_GEO_LAT_clean)
  ) %>%
  filter(
    is.finite(DEVICE_GEO_LONG_clean),
    is.finite(DEVICE_GEO_LAT_clean),
    is.finite(pred_prob)
  )

# Oregon state outline
or_state <- states(cb = TRUE, year = 2023, class = "sf") %>%
  filter(STUSPS == "OR") %>%
  st_transform(4326)

# Counties shapefile for Oregon
or_counties <- tigris::counties(state = "OR", cb = TRUE, year = 2023, class = "sf") %>%
  sf::st_transform(4326)

# Plot Model-Predicted Win Probability

ggplot() +
  # Oregon boundary
  geom_sf(data = or_state, fill = "gray95", color = "gray50", linewidth = 0.4) +
  # Oregon counties
  geom_sf(data = or_counties, fill = NA, color = "gray60", linewidth = 0.3) +
  # Predicted win-probability hex map
  stat_summary_hex(
    data = bids_pred_hex,
    aes(
      x = DEVICE_GEO_LONG_clean,
      y = DEVICE_GEO_LAT_clean,
      z = pred_prob
    ),
    fun  = mean,
    bins = 50,
    alpha = 0.85
  ) +

  # Color scale for predictions
  scale_fill_viridis_c(
    option = "plasma",
    name   = "Predicted\nWin Probability",
    limits = c(0, 1)
  ) +
  coord_sf(expand = FALSE) +
  labs(
    title = "Predicted Win Probability Across Oregon Cities",
    subtitle = "Spatial representation using Big Additive Model probability estimates",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", size = 15),
    plot.subtitle = element_text(size = 11),
    legend.position = "right"
  )

# convert back to char
bids_clean$BID_WON_clean <- ifelse(bids_clean$BID_WON_clean == 1, "TRUE", "FALSE")

```

### Plot Summary

The map shows how predicted win probabilities vary across Oregon. Higher probabilities cluster around major populated areas, especially in the northwest region, while many rural areas show lower or more scattered win chances. This suggests that auction competitiveness and bidding dynamics differ by location, with some cities consistently offering more favorable conditions for winning bids than others.


### **3. Auction Outcomes**

```{r}
#Library
library(dplyr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(lubridate)
library(forcats)
library(gridExtra)
library(ggrepel)


```

#1. Which features are most predictive of a win?

```{r}
# Define Cramér's V function
cramers_v <- function(x, y) {
  # Create contingency table
  confusion_matrix <- table(x, y)
  
  # Calculate chi-squared test
  chi2 <- chisq.test(confusion_matrix)
  
  # Get dimensions and sample size
  n <- sum(confusion_matrix)
  k <- min(dim(confusion_matrix))
  
  # Calculate Cramér's V
  v <- sqrt(chi2$statistic / (n * (k - 1)))
  
  return(as.numeric(v))
}

# Now run the categorical analysis again
cat("\n")
cat(strrep("=", 70), "\n")
cat("CATEGORICAL VARIABLE ANALYSIS WITH BID_WON\n")
cat(strrep("=", 70), "\n\n")

# Define categorical variables for analysis
categorical_vars <- c("DEVICE_TYPE_clean", "DEVICE_GEO_REGION_clean", 
                     "DEVICE_GEO_CITY_clean", "SIZE", "DAY_OF_WEEK")

# Prepare the data for correlation analysis
bids_analysis <- bids_clean %>%
  # Convert BID_WON to numeric (0/1)
  mutate(
    BID_WON_numeric = as.numeric(as.logical(BID_WON_clean)),
    
    # Extract date/time features
    HOUR = hour(TIMESTAMP_clean),
    DAY_OF_WEEK = wday(DATE_UTC_clean, label = TRUE, week_start = 1),
    DAY_OF_WEEK_NUM = as.numeric(DAY_OF_WEEK),
    MONTH = month(DATE_UTC_clean),
    DAY = day(DATE_UTC_clean),
    
    # Convert DEVICE_TYPE to factor
    DEVICE_TYPE_fct = as.factor(DEVICE_TYPE_clean),
    
    # Create ad size area from SIZE column
    SIZE_WIDTH = as.numeric(str_extract(SIZE, "^[0-9]+")),
    SIZE_HEIGHT = as.numeric(str_extract(SIZE, "[0-9]+$")),
    SIZE_AREA = SIZE_WIDTH * SIZE_HEIGHT,
    
    # Extract number of requested sizes
    NUM_REQUESTED_SIZES = map_int(REQUESTED_SIZES_clean, length),
    
    # Create region indicator
    REGION_OR = as.numeric(DEVICE_GEO_REGION_clean == "OR"),
    
    # Log-transform price for better distribution
    PRICE_log = log(PRICE_final + 0.001),
    
    # Log-transform response time
    RESPONSE_TIME_log = log(RESPONSE_TIME_clean + 1)
  )

categorical_results <- list()

for (var in categorical_vars) {
  if (var %in% names(bids_analysis)) {
    # Remove NA values for this analysis
    temp_data <- bids_analysis %>%
      select(!!sym(var), BID_WON_numeric) %>%
      filter(!is.na(!!sym(var))) %>%
      na.omit()
    
    if(nrow(temp_data) > 0 && length(unique(temp_data[[var]])) > 1) {
      # Calculate win rates
      win_rates <- temp_data %>%
        group_by(!!sym(var)) %>%
        summarise(
          count = n(),
          win_rate = mean(BID_WON_numeric, na.rm = TRUE),
          .groups = "drop"
        ) %>%
        arrange(desc(win_rate))
      
      # Calculate Cramér's V with error handling
      tryCatch({
        cramer_v <- cramers_v(temp_data[[var]], temp_data$BID_WON_numeric)
        
        cat("Variable:", var, "\n")
        cat("Cramér's V:", round(cramer_v, 4), "\n")
        cat("Sample size:", nrow(temp_data), "\n")
        cat("Unique categories:", length(unique(temp_data[[var]])), "\n")
        
        if(nrow(win_rates) <= 10) {
          cat("All categories:\n")
          print(win_rates)
        } else {
          cat("Top 5 categories by win rate:\n")
          print(head(win_rates, 5))
          cat("\nBottom 5 categories by win rate:\n")
          print(tail(win_rates, 5))
        }
        
        # Store results
        categorical_results[[var]] <- list(
          cramer_v = cramer_v,
          win_rates = win_rates,
          n = nrow(temp_data),
          n_categories = length(unique(temp_data[[var]]))
        )
        
      }, error = function(e) {
        cat("Variable:", var, "\n")
        cat("Could not calculate Cramér's V:", e$message, "\n")
      })
      
      cat("\n")
      cat(strrep("-", 50), "\n\n")
    } else {
      cat("Variable:", var, "\n")
      cat("Insufficient data or only one category\n\n")
      cat(strrep("-", 50), "\n\n")
    }
  }
}

# Let's also calculate some simpler metrics for categorical variables
cat("\n")
cat(strrep("=", 70), "\n")
cat("ALTERNATIVE CATEGORICAL ANALYSIS: RELATIVE WIN RATES\n")
cat(strrep("=", 70), "\n\n")

# For each categorical variable, calculate the range of win rates
for (var in categorical_vars) {
  if (var %in% names(bids_analysis)) {
    temp_data <- bids_analysis %>%
      select(!!sym(var), BID_WON_numeric) %>%
      filter(!is.na(!!sym(var))) %>%
      na.omit()
    
    if(nrow(temp_data) > 0 && length(unique(temp_data[[var]])) > 1) {
      win_summary <- temp_data %>%
        group_by(!!sym(var)) %>%
        summarise(
          count = n(),
          win_rate = mean(BID_WON_numeric, na.rm = TRUE),
          .groups = "drop"
        ) %>%
        filter(count > 10)  # Only consider categories with enough data
      
      if(nrow(win_summary) > 1) {
        win_range <- max(win_summary$win_rate) - min(win_summary$win_rate)
        best_category <- win_summary[which.max(win_summary$win_rate), ]
        worst_category <- win_summary[which.min(win_summary$win_rate), ]
        
        cat("Variable:", var, "\n")
        cat("Win rate range:", round(win_range, 4), "\n")
        cat("Best category:", best_category[[1]], 
            "(win rate:", round(best_category$win_rate, 4), 
            ", n:", best_category$count, ")\n")
        cat("Worst category:", worst_category[[1]], 
            "(win rate:", round(worst_category$win_rate, 4), 
            ", n:", worst_category$count, ")\n")
        cat("Relative difference:", round(best_category$win_rate / worst_category$win_rate, 2), "x\n")
        cat("\n")
      }
    }
  }
}

# Let's create a comprehensive summary table of all features
cat("\n")
cat(strrep("=", 70), "\n")
cat("COMPREHENSIVE FEATURE IMPORTANCE SUMMARY\n")
cat(strrep("=", 70), "\n\n")

# Collect all metrics in one data frame
feature_importance_summary <- data.frame()

# 1. Add numerical features from correlation
# First, create the numerical features dataset
numerical_features_clean <- bids_analysis %>%
  select(
    BID_WON_numeric,
    PRICE_final,
    PRICE_log,
    RESPONSE_TIME_clean,
    RESPONSE_TIME_log,
    SIZE_WIDTH,
    SIZE_HEIGHT,
    SIZE_AREA,
    NUM_REQUESTED_SIZES,
    HOUR,
    DAY_OF_WEEK_NUM,
    DAY,
    MONTH,
    DEVICE_GEO_LAT_clean,
    DEVICE_GEO_LONG_clean,
    REGION_OR
  ) %>%
  # Remove rows with any missing values for correlation
  na.omit() %>%
  # Remove infinite values if they exist
  mutate(across(everything(), ~ ifelse(is.infinite(.), NA, .))) %>%
  na.omit()

# Calculate correlation matrix
cor_matrix <- cor(numerical_features_clean, use = "complete.obs", method = "pearson")

# Get correlation with BID_WON
bid_won_correlations <- cor_matrix["BID_WON_numeric", ]
bid_won_cor_sorted <- sort(abs(bid_won_correlations), decreasing = TRUE)

# Create a nice formatted dataframe
bid_won_cor_df <- data.frame(
  Feature = names(bid_won_cor_sorted),
  Correlation = round(bid_won_correlations[names(bid_won_cor_sorted)], 4),
  Absolute_Correlation = round(bid_won_cor_sorted, 4)
) %>%
  filter(Feature != "BID_WON_numeric")  # Remove self-correlation


for (feature in bid_won_cor_df$Feature) {
  if(feature != "BID_WON_numeric") {
    cor_value <- bid_won_cor_df$Correlation[bid_won_cor_df$Feature == feature]
    feature_importance_summary <- rbind(feature_importance_summary, 
                                        data.frame(
                                          Feature = feature,
                                          Type = "Numerical",
                                          Metric = "Pearson Correlation",
                                          Value = abs(cor_value),
                                          Direction = ifelse(cor_value > 0, "Positive", "Negative"),
                                          Importance_Level = case_when(
                                            abs(cor_value) >= 0.3 ~ "High",
                                            abs(cor_value) >= 0.1 ~ "Medium",
                                            TRUE ~ "Low"
                                          )
                                        ))
  }
}

# 2. Add categorical features from Cramér's V
for (var in names(categorical_results)) {
  feature_importance_summary <- rbind(feature_importance_summary,
                                      data.frame(
                                        Feature = var,
                                        Type = "Categorical",
                                        Metric = "Cramér's V",
                                        Value = categorical_results[[var]]$cramer_v,
                                        Direction = "Variable",
                                        Importance_Level = case_when(
                                          categorical_results[[var]]$cramer_v >= 0.3 ~ "High",
                                          categorical_results[[var]]$cramer_v >= 0.1 ~ "Medium",
                                          TRUE ~ "Low"
                                        )
                                      ))
}

# Sort by importance
feature_importance_summary <- feature_importance_summary %>%
  arrange(desc(Value))

cat("ALL FEATURES RANKED BY PREDICTIVE POWER:\n")
cat(strrep("-", 70), "\n")
print(feature_importance_summary)
cat(strrep("-", 70), "\n\n")

# Create a visualization of feature importance
ggplot(feature_importance_summary, aes(x = reorder(Feature, Value), y = Value, fill = Importance_Level)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  geom_text(aes(label = round(Value, 3)), 
            hjust = -0.1, size = 3.5) +
  coord_flip() +
  scale_fill_manual(values = c("High" = "#E41A1C", "Medium" = "#377EB8", "Low" = "#4DAF4A")) +
  labs(
    title = "Feature Importance for Predicting Bid Wins",
    subtitle = "Higher values indicate stronger predictive power",
    x = "Feature",
    y = "Predictive Strength (Correlation or Cramér's V)",
    fill = "Importance Level"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))

  # Get device-specific win rates
  device_rates <- categorical_results$DEVICE_TYPE_clean$win_rates
  best_device <- device_rates[which.max(device_rates$win_rate), ]
  worst_device <- device_rates[which.min(device_rates$win_rate), ]
  
```
Conclusion Q1: 
PRICE IS KING:
   - Bid price has the strongest correlation with winning ( 0.487 )
   - Increasing bid price is the most reliable way to increase win probability



#2. Systematic differences across devices, regions, or ad categories
```{r}
cat("\n")
cat(strrep("=", 70), "\n")
cat(strrep("=", 70), "\n\n")

# PART 1: DEVICE TYPE DIFFERENCES

cat("1. DEVICE TYPE SYSTEMATIC DIFFERENCES\n")
cat(strrep("=", 70), "\n\n")

# 1.1 Comprehensive device analysis using raw DEVICE_TYPE_clean
cat("1.1 Device Type Performance Metrics (Raw Codes):\n")
cat(strrep("-", 70), "\n")

device_comprehensive <- bids_clean %>%
  filter(!is.na(DEVICE_TYPE_clean)) %>%
  group_by(DEVICE_TYPE_clean) %>%
  summarise(
    n_bids = n(),
    bid_share = n() / nrow(bids_clean) * 100,
    wins = sum(BID_WON_clean, na.rm = TRUE),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_bid_price = mean(PRICE_final, na.rm = TRUE),
    median_bid_price = median(PRICE_final, na.rm = TRUE),
    avg_response_time = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    std_dev_price = sd(PRICE_final, na.rm = TRUE),
    price_iqr = IQR(PRICE_final, na.rm = TRUE),
    # Winning price metrics
    avg_winning_price = mean(PRICE_final[BID_WON_clean], na.rm = TRUE),
    avg_losing_price = mean(PRICE_final[!BID_WON_clean], na.rm = TRUE),
    price_premium = avg_winning_price - avg_losing_price,
    # Additional metrics
    min_price = min(PRICE_final, na.rm = TRUE),
    max_price = max(PRICE_final, na.rm = TRUE),
    q25_price = quantile(PRICE_final, 0.25, na.rm = TRUE),
    q75_price = quantile(PRICE_final, 0.75, na.rm = TRUE)
  ) %>%
  arrange(desc(win_rate))

print(device_comprehensive)
cat("\n")

# 1.2 Device type statistical tests
cat("1.2 Statistical Tests for Device Differences:\n")
cat(strrep("-", 70), "\n")

# Filter for device types with sufficient data
device_types_sufficient <- device_comprehensive %>%
  filter(n_bids >= 100) %>%
  pull(DEVICE_TYPE_clean)

device_test_data <- bids_clean %>%
  filter(DEVICE_TYPE_clean %in% device_types_sufficient) %>%
  mutate(DEVICE_TYPE_clean = factor(DEVICE_TYPE_clean))

# ANOVA for price differences
if (length(device_types_sufficient) >= 2) {
  price_anova <- aov(PRICE_final ~ DEVICE_TYPE_clean, data = device_test_data)
  cat("ANOVA - Price Differences by Device Type:\n")
  print(summary(price_anova))

  # Post-hoc test if significant
  if (summary(price_anova)[[1]]$`Pr(>F)`[1] < 0.05) {
    cat("\nTukey HSD Post-hoc Test (Price):\n")
    print(TukeyHSD(price_anova))
  }
}

# Kruskal-Wallis test for win rate
win_kruskal <- kruskal.test(as.numeric(BID_WON_clean) ~ DEVICE_TYPE_clean,
                           data = device_test_data)
cat("\nKruskal-Wallis Test - Win Rate Differences by Device Type:\n")
cat(sprintf("H = %.2f, p-value = %.6f\n",
            win_kruskal$statistic, win_kruskal$p.value))

# Chi-square test for win distribution
device_contingency <- table(device_test_data$DEVICE_TYPE_clean,
                           device_test_data$BID_WON_clean)
chi_device <- chisq.test(device_contingency)
cat("\nChi-square Test - Win Distribution by Device Type:\n")
cat(sprintf("X-squared = %.2f, df = %d, p-value = %.6f\n",
            chi_device$statistic, chi_device$parameter, chi_device$p.value))
cat("\n")

# 1.3 Device type time patterns
cat("1.3 Device Usage Patterns by Time of Day:\n")
cat(strrep("-", 70), "\n")

device_time_patterns <- bids_clean %>%
  filter(DEVICE_TYPE_clean %in% device_types_sufficient) %>%
  mutate(
    hour = hour(TIMESTAMP_clean),
    day_part = case_when(
      hour >= 6 & hour < 12 ~ "Morning",
      hour >= 12 & hour < 18 ~ "Afternoon",
      hour >= 18 & hour < 24 ~ "Evening",
      hour >= 0 & hour < 6 ~ "Night"
    )
  ) %>%
  group_by(DEVICE_TYPE_clean, day_part) %>%
  summarise(
    n_bids = n(),
    share_of_device = n() / sum(n()) * 100,
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    avg_response = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_wider(
    names_from = day_part,
    values_from = c(n_bids, share_of_device, win_rate, avg_price, avg_response)
  )

print(device_time_patterns)
cat("\n")
```


```{r}
# PART 2: REGIONAL DIFFERENCES (RAW DATA)

cat("\n2. REGIONAL (GEOGRAPHIC) SYSTEMATIC DIFFERENCES\n")
cat(strrep("=", 70), "\n\n")

# 2.1 City-level comprehensive analysis using raw DEVICE_GEO_CITY_clean
cat("2.1 Top 15 Cities by Bid Volume:\n")
cat(strrep("-", 70), "\n")

city_top15 <- bids_clean %>%
  filter(!is.na(DEVICE_GEO_CITY_clean)) %>%
  group_by(DEVICE_GEO_CITY_clean) %>%
  summarise(
    n_bids = n(),
    wins = sum(BID_WON_clean, na.rm = TRUE),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    median_price = median(PRICE_final, na.rm = TRUE),
    avg_response_time = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    mobile_pct = mean(DEVICE_TYPE_clean == "1", na.rm = TRUE) * 100,
    desktop_pct = mean(DEVICE_TYPE_clean == "4", na.rm = TRUE) * 100,
    device_2_pct = mean(DEVICE_TYPE_clean == "2", na.rm = TRUE) * 100,
    # Price distribution metrics
    price_sd = sd(PRICE_final, na.rm = TRUE),
    price_min = min(PRICE_final, na.rm = TRUE),
    price_max = max(PRICE_final, na.rm = TRUE),
    price_q1 = quantile(PRICE_final, 0.25, na.rm = TRUE),
    price_q3 = quantile(PRICE_final, 0.75, na.rm = TRUE),
    price_iqr = price_q3 - price_q1,
    # Winning metrics
    winning_avg_price = mean(PRICE_final[BID_WON_clean], na.rm = TRUE),
    losing_avg_price = mean(PRICE_final[!BID_WON_clean], na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  filter(n_bids >= 100) %>%
  mutate(
    price_cv = price_sd / avg_price * 100
  ) %>%
  arrange(desc(n_bids)) %>%
  head(15)

print(city_top15)
cat("\n")

# 2.2 Regional statistical tests
cat("2.2 Statistical Tests for Regional Differences:\n")
cat(strrep("-", 70), "\n")

# Select top 5 cities for analysis
top5_cities <- city_top15 %>%
  head(5) %>%
  pull(DEVICE_GEO_CITY_clean)

regional_test_data <- bids_clean %>%
  filter(DEVICE_GEO_CITY_clean %in% top5_cities) %>%
  mutate(city = factor(DEVICE_GEO_CITY_clean))

# ANOVA for price differences
if (n_distinct(regional_test_data$city) >= 2) {
  regional_price_anova <- aov(PRICE_final ~ city, data = regional_test_data)
  cat("ANOVA - Price Differences by City:\n")
  print(summary(regional_price_anova))

  if (summary(regional_price_anova)[[1]]$`Pr(>F)`[1] < 0.05) {
    cat("\nTukey HSD Post-hoc Test (Regional Price Differences):\n")
    print(TukeyHSD(regional_price_anova))
  }
}

# Kruskal-Wallis for win rate differences
regional_win_kruskal <- kruskal.test(as.numeric(BID_WON_clean) ~ city,
                                    data = regional_test_data)
cat("\nKruskal-Wallis Test - Win Rate Differences by City:\n")
cat(sprintf("H = %.2f, p-value = %.6f\n",
            regional_win_kruskal$statistic, regional_win_kruskal$p.value))

# Chi-square for win distribution
regional_contingency <- table(regional_test_data$city,
                             regional_test_data$BID_WON_clean)
chi_region <- chisq.test(regional_contingency)
cat("\nChi-square Test - Win Distribution by City:\n")
cat(sprintf("X-squared = %.2f, df = %d, p-value = %.6f\n",
            chi_region$statistic, chi_region$parameter, chi_region$p.value))
cat("\n")

# 2.3 City performance by device type
cat("2.3 City Performance by Device Type:\n")
cat(strrep("-", 70), "\n")

city_device_performance <- bids_clean %>%
  filter(DEVICE_GEO_CITY_clean %in% top5_cities &
         DEVICE_TYPE_clean %in% device_types_sufficient) %>%
  group_by(DEVICE_GEO_CITY_clean, DEVICE_TYPE_clean) %>%
  summarise(
    n_bids = n(),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    avg_response = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_wider(
    names_from = DEVICE_TYPE_clean,
    values_from = c(n_bids, win_rate, avg_price, avg_response),
    names_sep = "_"
  )

print(city_device_performance)
cat("\n")
```


```{r}
# PART 3: AD SIZE DIFFERENCES (RAW DATA)

cat("\n3. AD SIZE SYSTEMATIC DIFFERENCES\n")
cat(strrep("=", 70), "\n\n")

# 3.1 Ad size comprehensive analysis using raw SIZE
cat("3.1 Top 20 Ad Sizes by Volume:\n")
cat(strrep("-", 70), "\n")

size_analysis <- bids_clean %>%
  filter(!is.na(SIZE)) %>%
  group_by(SIZE) %>%
  summarise(
    n_bids = n(),
    wins = sum(BID_WON_clean, na.rm = TRUE),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    median_price = median(PRICE_final, na.rm = TRUE),
    avg_response_time = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    price_sd = sd(PRICE_final, na.rm = TRUE),
    # Device distribution
    mobile_pct = mean(DEVICE_TYPE_clean == "1", na.rm = TRUE) * 100,
    desktop_pct = mean(DEVICE_TYPE_clean == "4", na.rm = TRUE) * 100,
    device_2_pct = mean(DEVICE_TYPE_clean == "2", na.rm = TRUE) * 100,
    # Price range
    min_price = min(PRICE_final, na.rm = TRUE),
    max_price = max(PRICE_final, na.rm = TRUE),
    q25_price = quantile(PRICE_final, 0.25, na.rm = TRUE),
    q75_price = quantile(PRICE_final, 0.75, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  filter(n_bids >= 10) %>%
  arrange(desc(n_bids)) %>%
  head(20)

print(size_analysis)
cat("\n")

# 3.2 Statistical tests for size differences
cat("3.2 Statistical Tests for Size Differences:\n")
cat(strrep("-", 70), "\n")

# Test for common sizes (n >= 1000)
common_sizes <- size_analysis %>%
  filter(n_bids >= 1000) %>%
  pull(SIZE)

if (length(common_sizes) >= 2) {
  size_test_data <- bids_clean %>%
    filter(SIZE %in% common_sizes) %>%
    mutate(SIZE = factor(SIZE))

  # ANOVA for price differences
  size_price_anova <- aov(PRICE_final ~ SIZE, data = size_test_data)
  cat("ANOVA - Price Differences by Ad Size:\n")
  print(summary(size_price_anova))

  # Kruskal-Wallis for win rate
  size_win_kruskal <- kruskal.test(as.numeric(BID_WON_clean) ~ SIZE,
                                  data = size_test_data)
  cat("\nKruskal-Wallis Test - Win Rate Differences by Ad Size:\n")
  cat(sprintf("H = %.2f, p-value = %.6f\n",
              size_win_kruskal$statistic, size_win_kruskal$p.value))

  # Chi-square for win distribution
  size_contingency <- table(size_test_data$SIZE,
                           size_test_data$BID_WON_clean)
  chi_size <- chisq.test(size_contingency)
  cat("\nChi-square Test - Win Distribution by Ad Size:\n")
  cat(sprintf("X-squared = %.2f, df = %d, p-value = %.6f\n",
              chi_size$statistic, chi_size$parameter, chi_size$p.value))
}
cat("\n")

# 3.3 Size performance by device type
cat("3.3 Size Performance by Device Type (Common Sizes):\n")
cat(strrep("-", 70), "\n")

if (length(common_sizes) >= 1) {
  size_device_performance <- bids_clean %>%
    filter(SIZE %in% common_sizes[1:3] &  # Top 3 common sizes
           DEVICE_TYPE_clean %in% device_types_sufficient) %>%
    group_by(SIZE, DEVICE_TYPE_clean) %>%
    summarise(
      n_bids = n(),
      win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
      avg_price = mean(PRICE_final, na.rm = TRUE),
      avg_response = mean(RESPONSE_TIME_clean, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    pivot_wider(
      names_from = DEVICE_TYPE_clean,
      values_from = c(n_bids, win_rate, avg_price, avg_response),
      names_sep = "_"
    )

  print(size_device_performance)
}
cat("\n")
```





```{r}
# PART 5: TIME-BASED ANALYSIS

cat("\n5. TIME-BASED SYSTEMATIC DIFFERENCES\n")
cat(strrep("=", 70), "\n\n")

# 5.1 Hour of day analysis
cat("5.1 Hourly Analysis of Auction Performance:\n")
cat(strrep("-", 70), "\n")

hourly_analysis <- bids_clean %>%
  mutate(hour = hour(TIMESTAMP_clean)) %>%
  group_by(hour) %>%
  summarise(
    n_bids = n(),
    wins = sum(BID_WON_clean, na.rm = TRUE),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    median_price = median(PRICE_final, na.rm = TRUE),
    avg_response = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    mobile_pct = mean(DEVICE_TYPE_clean == "1", na.rm = TRUE) * 100,
    desktop_pct = mean(DEVICE_TYPE_clean == "4", na.rm = TRUE) * 100,
    .groups = 'drop'
  ) %>%
  arrange(hour)

print(hourly_analysis)
cat("\n")

# 5.2 Day of week analysis
cat("5.2 Day of Week Analysis:\n")
cat(strrep("-", 70), "\n")

weekday_analysis <- bids_clean %>%
  mutate(
    weekday = wday(DATE_UTC_clean, label = TRUE, abbr = FALSE),
    weekday_num = wday(DATE_UTC_clean)
  ) %>%
  group_by(weekday, weekday_num) %>%
  summarise(
    n_bids = n(),
    wins = sum(BID_WON_clean, na.rm = TRUE),
    win_rate = mean(BID_WON_clean, na.rm = TRUE) * 100,
    avg_price = mean(PRICE_final, na.rm = TRUE),
    median_price = median(PRICE_final, na.rm = TRUE),
    avg_response = mean(RESPONSE_TIME_clean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(weekday_num)

print(weekday_analysis %>% select(-weekday_num))
cat("\n")

# 5.3 Time-based statistical tests
cat("5.3 Statistical Tests for Time Differences:\n")
cat(strrep("-", 70), "\n")

# Prepare data for time analysis
time_data <- bids_clean %>%
  mutate(
    hour = hour(TIMESTAMP_clean),
    day_part = case_when(
      hour >= 6 & hour < 12 ~ "Morning",
      hour >= 12 & hour < 18 ~ "Afternoon",
      hour >= 18 & hour < 24 ~ "Evening",
      hour >= 0 & hour < 6 ~ "Night"
    )
  )

# ANOVA for price by day part
if (n_distinct(time_data$day_part) >= 2) {
  time_price_anova <- aov(PRICE_final ~ day_part, data = time_data)
  cat("ANOVA - Price Differences by Time of Day:\n")
  print(summary(time_price_anova))
}

# Kruskal-Wallis for win rate by day part
time_win_kruskal <- kruskal.test(as.numeric(BID_WON_clean) ~ day_part, data = time_data)
cat("\nKruskal-Wallis Test - Win Rate Differences by Time of Day:\n")
cat(sprintf("H = %.2f, p-value = %.6f\n",
            time_win_kruskal$statistic, time_win_kruskal$p.value))
cat("\n")
```

```{r}
# PART 6: MULTIVARIATE ANALYSIS

cat("\n6. MULTIVARIATE ANALYSIS\n")
cat(strrep("=", 70), "\n\n")

# 6.1 Prepare data for modeling
cat("6.1 Linear Model: Factors Affecting Bid Price\n")
cat(strrep("-", 70), "\n")

# Create modeling dataset with raw variables
model_data <- bids_clean %>%
  filter(!is.na(PRICE_final) &
         !is.na(DEVICE_TYPE_clean) &
         !is.na(DEVICE_GEO_CITY_clean) &
         !is.na(SIZE)) %>%
  # Create indicator variables
  mutate(
    Type_one = as.numeric(DEVICE_TYPE_clean == "1"),
    Type_four = as.numeric(DEVICE_TYPE_clean == "4"),
    Type_two = as.numeric(DEVICE_TYPE_clean == "2"),
    in_top_city = as.numeric(DEVICE_GEO_CITY_clean %in% top5_cities),
    is_common_size = as.numeric(SIZE %in% common_sizes[1:3]),
    hour_of_day = hour(TIMESTAMP_clean),
    log_price = log(PRICE_final + 0.001),
    log_response = log(RESPONSE_TIME_clean + 1)
  ) %>%
  # Take a sample if data is too large
  { if (nrow(.) > 50000) sample_n(., 50000) else . }

# Fit linear model for price
if (nrow(model_data) > 100) {
  price_model <- lm(log_price ~ Type_one + Type_four +
                 Type_two + in_top_city + is_common_size
                 + hour_of_day + log_response, data = model_data)

  cat("Linear Model Summary (Log Price):\n")
  print(summary(price_model))
}

# 6.2 Logistic regression for win probability
cat("\n6.2 Logistic Regression: Factors Affecting Win Probability\n")
cat(strrep("-", 70), "\n")

if (nrow(model_data) > 100) {
  win_model <- glm(BID_WON_clean ~ PRICE_final + Type_one
                   + Type_four + Type_two + in_top_city +
                     is_common_size + hour_of_day +
                     RESPONSE_TIME_clean, family = binomial(), data = model_data)

  cat("Logistic Model Summary (Win Probability):\n")
  print(summary(win_model))

  # Calculate odds ratios
  odds_ratios <- exp(coef(win_model))
  cat("\nOdds Ratios (Multiplicative Effect on Win Odds):\n")
  print(round(odds_ratios, 3))
}
cat("\n")
```
Conclusion: The result of P-value and Odds Ratios will change a little bit during every run, since I randomly pick part of these data. But we can find that Price is highly significant systematic difference. Most Device type has significant systematic difference, but p-value of the Type_four is around alpha=0.05. Region and device size has significant systematic difference. The hour in day has less systematic difference since odds ratios is close to 1. The Response time has no systematic difference since p-value > 0.05 and odds ratios = 1



### **4. Geographic Analysis**
* Visual reprsentation of the data in a geographic map
* Q: how does the proximity to portland affect the price of a bid?

* quick comparison of price between raw and cleaned data. We see there is no real difference in the mean, meaning that the outliers did not affect the overall trends of the data too much.
# price

### Comparison between uncleaned and cleaned price data
This is not necessarily needed for the presentation. Just was central to the original Q4. Just shows the data we fixed did not really change the statistics
```{r}
library(hexbin)
library(geosphere)

bids_clean_2 <- bids_clean

col_comparison <- function(df_1, df_2, col_1, col_2, label_1 = "Before", label_2 = "After") {
  bind_rows(
    df_1 %>%
      summarise(
        mean = mean(.data[[col_1]], na.rm = TRUE),
        sd = sd(.data[[col_1]], na.rm = TRUE),
        min = min(.data[[col_1]], na.rm = TRUE),
        max = max(.data[[col_1]], na.rm = TRUE),
        n = n(),
        na_count = sum(is.na(.data[[col_1]]))
      ) %>%
      mutate(dataset = label_1, .before = 1),

    df_2 %>%
      summarise(
        mean = mean(.data[[col_2]], na.rm = TRUE),
        sd = sd(.data[[col_2]], na.rm = TRUE),
        min = min(.data[[col_2]], na.rm = TRUE),
        max = max(.data[[col_2]], na.rm = TRUE),
        n = n(),
        na_count = sum(is.na(.data[[col_2]]))
      ) %>%
      mutate(dataset = label_2, .before = 1)
  )
}

col_comparison(bids, bids_clean, "PRICE_clean", "PRICE_final")

```


# Price Distance to portland (grouped)
broke oregon into 60 hexes and grouped data. Looked at average of bid price and winning big price of each hex. Plotted average bid and winning bid price vs distance to portland.
```{r}
#------------------------------------------------------------------------
# Group Zips into Hexes and categorize hex by city with most frequent
# zip per hex
#------------------------------------------------------------------------
bids_clean <- bids_clean_2


hb <- hexbin(
  x = bids_clean$DEVICE_GEO_LONG_clean,
  y = bids_clean$DEVICE_GEO_LAT_clean,
  xbins = 60,  # same as bins in ggplot
  IDs = TRUE
)

# Get cell assignments for each point
bids_clean$hex_id <- hb@cID
zip_city_lookup <- zipcodeR::zip_code_db %>%
  filter(state == "OR") %>%
  select(zipcode, major_city)

# Find top ZIP per hex and join city names
hex_with_city <- bids_clean %>%
  # Remove rows where hex_id or ZIP is missing
  filter(!is.na(hex_id), !is.na(DEVICE_GEO_ZIP_clean)) %>%
  # Count how many bids per hex + ZIP combination
  count(hex_id, DEVICE_GEO_ZIP_clean, name = "zip_count") %>%
  # Keep only the most frequent ZIP for each hex (1 row per hex)
  slice_max(zip_count, n = 1, by = hex_id, with_ties = FALSE) %>%
  # Attach city name by matching ZIP to zipcodeR lookup table
  left_join(zip_city_lookup, by = c("DEVICE_GEO_ZIP_clean" = "zipcode")) %>%
  # Keep only the columns we need for the final join
  select(hex_id, zip_count, major_city)

# Join to bids
bids_clean <- bids_clean %>%
  left_join(hex_with_city, by = "hex_id")

ggplot() +
  geom_sf(data = zip_code_db, fill = "white", color = "gray80") +
  stat_summary_hex(data = bids_clean,
                   aes(x = DEVICE_GEO_LONG_clean, y = DEVICE_GEO_LAT_clean, z = PRICE_final),
                   fun = mean, bins = 30, alpha = 0.8) +
  theme_minimal()
```

# Price vs Distance to Portland (individual bids)
similar to above but did not group bids into regions. Just plotted each bid and calculated the distance to portland for each based on lat/long
```{r}
#------------------------------------------------------------------------
# Calculate distance each hex is from Portland's hex
#------------------------------------------------------------------------

# Step 1: Get hex centers from the hexbin object
hex_centers <- tibble(
  hex_id = hb@cell,
  hex_x = hb@xcm,  # longitude center
  hex_y = hb@ycm   # latitude center
)

# Step 2: Find Portland's hex (by city name or coordinates)
# Option A: By city name (if you have major_city in hex_with_city)
portland_hex_id <- hex_with_city %>%
  filter(major_city == "Portland") %>%
  pull(hex_id) %>%
  first()

# Step 3: Get Portland's hex center
portland_center <- hex_centers %>%
  filter(hex_id == portland_hex_id)

hex_distances <- hex_centers %>%
  rowwise() %>%
  mutate(
    dist_to_portland_km = distHaversine(
      c(hex_x, hex_y),                              # this hex
      c(portland_center$hex_x, portland_center$hex_y)  # Portland hex
    ) / 1000  # meters to km
  ) %>%
  ungroup()

# Step 5: Join back to bids_clean
bids_clean <- bids_clean %>%
  left_join(hex_distances %>% select(hex_id, dist_to_portland_km), by = "hex_id")

# Calculate mean price per hex with distance
hex_price_distance_all <- bids_clean %>%
  filter(!is.na(dist_to_portland_km), !is.na(PRICE_final)) %>%
  group_by(hex_id, dist_to_portland_km) %>%
  summarise(
    avg_price = mean(PRICE_final, na.rm = TRUE),
    n_bids = n(),
    .groups = "drop"
  )


# Calculate mean price per hex with distance
hex_price_distance_winning <- bids_clean %>%
  filter(!is.na(dist_to_portland_km), !is.na(PRICE_final), BID_WON_clean == TRUE) %>%
  group_by(hex_id, dist_to_portland_km) %>%
  summarise(
    avg_price = mean(PRICE_final, na.rm = TRUE),
    n_bids = n(),
    .groups = "drop"
  )

p1 <- ggplot(hex_price_distance_all, aes(x = dist_to_portland_km, y = avg_price)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Average Bid Price vs Distance from Portland",
    x = "Distance from Portland (km)",
    y = "Average Price"
  ) +
  scale_fill_viridis_d(option = "cividis") +
  theme_minimal()

p2 <- ggplot(hex_price_distance_winning, aes(x = dist_to_portland_km, y = avg_price)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Average Winning Bid Price vs Distance from Portland",
    x = "Distance from Portland (km)",
    y = "Average Price"
  ) +
  scale_fill_viridis_d(option = "cividis") +
  theme_minimal()

p1
p2


#------------------------------------------------------------------------
# Calculate distance each bid lat/long is from Portland
#------------------------------------------------------------------------

# Portland center coordinates
portland_lat <- 45.52
portland_lon <- -122.68

bids_clean <- bids_clean %>%
  mutate(
    dist_to_portland_row_km_bid = distHaversine(
      cbind(DEVICE_GEO_LONG_clean, DEVICE_GEO_LAT_clean),
      c(portland_lon, portland_lat)
    ) / 1000
  )

ggplot(bids_clean %>% filter(!is.na(dist_to_portland_row_km_bid), !is.na(PRICE_final), BID_WON_clean == TRUE) %>% sample_n(10000),
             aes(x = dist_to_portland_row_km, y = PRICE_final)) +
  geom_point(alpha = 0.1, size = 0.3) +
  geom_smooth(method = "loess", color = "red") +
  labs(title = "Individual Winning Bids (10k sample)", x = "Distance (km)", y = "Price") +
  theme_minimal()






```

#

```{r}

```


#Data Visualization

#Synthesis

*Notes: Things to make clear in the sythesis portion of this all for our own use:

  >What were points of uncertainity? What steps did we take to address them and how could they still be showing     up?
  >How did we clean and prepare the data set for our work? What choices did we make and how did we standardize?

#Presentation Assignment:

##We will decide who is generating what info and transferring it to the slides.
##We also need to decide who will present what and how to field questions.

Final presentation ask:
Your final presentation, in **20–30 minutes**, should tell a clear, professional story of your team’s
data-cleaning workflow, exploratory analysis, and collaboration practices. The goal is to
demonstrate not only what you found, but how you worked as a data science team.

Things we need to include in the final presentation:

Key Issues You Found
Examples include:
• Missingness patterns
• Formatting inconsistencies (dates, numeric/character mismatches, categorical typos)
• Duplicates
• Outliers or implausible values
• Structural issues (names, types, consistency)

Your Cleaning Strategy
For each issue:
• What you observed
• Why it was a problem
• Principle behind your fix (e.g., “We chose this imputation strategy because…”)
• Concise R code or pseudo-code
Focus on justifying decisions, not on full code dumps.

Reproducibility and Workflow
Highlight:
• Git usage (branches, PRs, merge conflicts)
• Script modularity and readability
• R Markdown / Quarto documentation
• Naming conventions and folder structure

EDA
Overview Patterns
Show:
• Distributions
• Key relationships
• Missingness profiles
• Surprising patterns

Deep Dives on Guiding Questions
For each:
• State the question
• Show relevant figures
• Interpret results clearly
• Connect to cleaning decisions

Visualization Quality
Ensure:
• Clear labels/titles
• Good color choices
• No clutter
• Interpretation accompanies each visual

Insights
Summarize:
• 3–5 most important findings
• What the data suggests overall
• Uncertainties or next steps

Collaboration Reflection
Discuss:
• What went well in your workflow
• What was challenging
• Lessons learned for future projects
• How GitHub, Jira, and RStudio supported collaboration

GitHub Repository Checklist
Confirm your repo includes:
• Clean README
• Data cleaning scripts
• EDA scripts (.qmd or .Rmd)
• Clear folder structure (data/, R/, figs/)
• Kanban snapshot
• Evidence of teamwork (commit history, pull requests, etc.)


