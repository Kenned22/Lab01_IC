---
title: "Lab03: Data Cleaning with R"
author: "International Commuters"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

# Overview & Goals

In this lab, you will **continue working with the web advertising bids data** to:

- Systematically **detect** and **repair** some other data quality problems using those ideas.
- Create a **cleaned version** of the dataset (e.g., `bids_clean`).
- Document your work in a **data cleaning log**.

By the end, you should have:

1. A data set with the majority of the insidious issues corrected.
2. A short narrative (or table) explaining **what you fixed** and **how**.

> *Work in the **same repository** as previous labs. Update your Jira Kanban board with new cards for this lab and keep committing your progress with clear messages.*

# Setup

## Packages

Minimally, you will need these packages:

```{r loadpackages}
library(tidyverse)
library(arrow)

library(logger)
library(glue)
library(dplyr)
library(tidyr)
library(rlang)
library(lubridate)
library(tictoc)
library(here)
library(jsonlite)
library(knitr)
library(kableExtra)
library(DT)

# source(here("src", "data_cleaning.r"))
```

- `tidyverse` (especially `dplyr`, `ggplot2`, `readr`, `stringr`)
- `stringr` to modify character strings
- `lubridate` for date–time parsing
- `arrow` if you are reading/writing Parquet files

> **Task:** In a code chunk, load the packages you need.

## Load the messy bids data

Use the **same dataset** as in the earlier lab (e.g., a Parquet file in your `data/` folder).

> **Tasks:**
>
> 1. Read the messy data into R (e.g., using `arrow::read_parquet()` or `readr` functions, depending on your file).
> 2. Use `dplyr::glimpse()` and/or `summary()` to remind yourselves what variables are present and roughly what they look like.
> 3. Create a **working copy** of the data so that you can always go back to the original if needed.

You may want to keep the original as `bids_raw` and your working copy as `bids`.

```{r}
# Your code here

original_bids <- read_parquet(here("data", "bids_data_vDTR.parquet"))
bids <- original_bids %>% mutate(row_id = row_number())

expected_columns_path <- here("src", "expected_columns.csv")

if (file.exists(expected_columns_path)) {
  expected_columns <- data.frame(readr::read_csv(
    expected_columns_path,
    col_types = "ccc"
  ))
} else {
  expected_columns <- data.frame(
    column_name = c("id", "value", "timestamp"),
    expected_type = c("integer", "double", "datetime"),
    description = c("Unique ID", "Numeric Value", "Time Recorded")
  )
  readr::write_csv(expected_columns, expected_columns_path)
  message("expected_columns.csv was not found, so a default version was created.")
}

glimpse(bids)

```


# Quick Orientation Check (10–15 minutes)

Using your prior lab work and Data Cleaning lectures:

1. In **2–4 sentences**, describe **what each row** in this dataset represents.
*Each row represents a single bid in an online advertising auction. It captures the bid's details, including the auction context, user device information, geographic data, ad specifications, and the bid's monetary and technical parameters.*

2. Identify **one or two variables** that you think should be:
   - **Mandatory** (must not be missing), and
   - Possibly a **unique key** or part of a composite key.
*Mandatory: `TIMESTAMP` = When it happened, `PRICE` = The business value*
*Unique key: As confirmed in the analysis, no natural composite key (e.g., `AUCTION_ID` + `PUBLISHER_ID`) is truly unique, as duplicate combinations exist. Therefore, the artificially added `row_id` column is necessary to serve as a unique key for each bid record.*

3. Pick **two variables** and, for each, assign at least one **data quality dimension** (e.g., completeness, validity, consistency, uniqueness, cross-field consistency).
*`PRICE` (Validity & Completeness)*
*`RESPONSE_TIME` (Validity & Consistency)*

> **Write your answers directly in this Rmd in a text section (not code).**

---

NEAL:

I already did some work on this and have a summary function that looks at the bids df and the
expected columns, as reported in the expected_columns.csv file. Can follow up with written explanation.

I already answered the unique key question in lab 2. There is no set of cols that I feel confident can be used
as a unique key. I added a row_id column to the bids dataframe to use as a unique key.

I made a slight change to the code to help it run even if the expected csv is not there, so far work
that you've done Neal looks great!

---

```{r column check and type summary functions}
#' Check if expected columns exist in a data frame
#'
#' @param df A data frame to check
#' @param expected_columns Character vector of expected column names
#' @return Character vector of missing column names (invisible)
#' @examples
#' check_columns(df, c("id", "name", "age"))
check_columns <- function(df, expected_columns) {
  if (!is.list(expected_columns) && !is.character(expected_columns)) {
    msg <- "expected_columns must be a list or character vector"
    log_error(msg)
    stop(msg)
  }

  expected_columns <- as.character(expected_columns)
  missing_columns <- setdiff(expected_columns, colnames(df))

  if (length(missing_columns) == length(expected_columns)) {
    msg <- "All expected columns are missing from the dataframe"
    log_error(msg)
    stop(msg)
  } else if (length(missing_columns) > 0) {
    warning(glue("Missing columns:\n  {paste(missing_columns, collapse = '\n  ')}"))
  }

  missing_columns
}

#' Check column types against expected types
#'
#' @param df A data frame to check
#' @param expected_types A data frame with columns: 'column', 'Expected_Type'
#' @return A data frame comparing actual vs expected types
#' @examples
#' expected <- data.frame(
#'   column = c("age", "name"),
#'   Expected_Type = c("numeric", "character")
#' )
#' check_column_types(df, expected)
check_column_types <- function(df, expected_types) {
  if (is.null(expected_types$column) || length(expected_types$column) == 0) {
    stop("expected_types must have a 'column' field with column names")
  }

  if (!all(sapply(expected_types$Expected_Type, is.character))) {
    stop("All values in Expected_Type must be character strings")
  }

  # Get actual types
  df_types <- sapply(df, function(col) class(col)[1])

  # Find common columns
  common_cols <- intersect(names(df), expected_types$column)

  # Create comparison
  type_comparison <- expected_types %>%
    filter(.data$column %in% common_cols) %>%
    mutate(
      actual = df_types[.data$column],
      .before = "Expected_Type"
    ) %>%
    mutate(
      match = .data$actual == .data$Expected_Type,
      .after = "Expected_Type"
    )

  type_comparison
}


```


```{r type summary}
missing_columns <- check_columns(bids, expected_columns$column)
cat(glue::glue("There are {length(missing_columns)} missing column(s): \n {paste(missing_columns, collapse = ', ')}"))

bids_type_summary <- check_column_types(bids,  expected_columns)
print(bids_type_summary)

comparison %>%
  kable("html", escape = FALSE, align = c("l", rep("c", ncol(comparison) - 1))) %>%
  row_spec(which(!comparison$match), background = "#62ec0001") %>%
  column_spec(seq_len(ncol(comparison)), extra_css = "padding-right: 30px; padding-left: 20px;") %>% # nolint
  kable_styling(full_width = FALSE)

```

```{r uniquness example}
ex <- bids %>% group_by(AUCTION_ID, PUBLISHER_ID) %>%  filter(n() > 1) %>% head(5)
DT::datatable(ex,
              options = list(scrollX = TRUE, scrollY = "300px", pageLength = 5),
              class = "compact stripe") %>%
  formatStyle(
    columns = names(ex),
    backgroundColor = "white",
    color = "black"
  )

```


```{r Functions: Type conversion helper functions}

# ==============================================================================
# TYPE CONVERSION HELPERS
# ==============================================================================

#' Get type conversion function for a target type
#'
#' @param target_type Character string of target type
#' @return Function that converts to target type
get_type_func <- function(target_type) {
  converters <- list(
    character = as.character,
    numeric = as.numeric,
    integer = as.integer,
    logical = as.logical,
    Date = as.Date,
    POSIXct = function(x, ...) as.POSIXct(x, ...)
  )

  if (!target_type %in% names(converters)) {
    stop(glue("Unknown type: {target_type}\nSupported: {paste(names(converters), collapse = ', ')}"))
  }

  converters[[target_type]]
}

#' Get type checking function for a target type
#'
#' @param target_type Character string of target type
#' @return Function that checks if value is of target type
get_check_func <- function(target_type) {
  checkers <- list(
    character = is.character,
    numeric = is.numeric,
    integer = is.integer,
    logical = is.logical,
    Date = function(x) inherits(x, "Date"),
    POSIXct = function(x) inherits(x, "POSIXct")
  )

  if (!target_type %in% names(checkers)) {
    stop(glue("Unknown type: {target_type}"))
  }

  checkers[[target_type]]
}

# ==============================================================================
# GENERIC TYPE CONVERSION FUNCTION
# ==============================================================================

#' Convert a column to a target type with optional preprocessing
#'
#' @param df A data frame
#' @param col_name Name of the column to convert
#' @param target_type Target type as string (e.g., "numeric", "POSIXct")
#' @param preprocess_fn Optional preprocessing function
#' @param verbose Whether to print conversion messages
#' @param ... Additional arguments passed to conversion function
#' @return Data frame with converted column
#' @examples
#' # Convert with preprocessing
#' df <- convert_column(df, "PRICE", "numeric",
#'                     preprocess_fn = function(x) gsub("^O", "0", x))
#'
#' # Convert with format specification
#' df <- convert_column(df, "TIMESTAMP", "POSIXct",
#'                     format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
convert_column <- function(df, col_name, target_type,
                           preprocess_fn = NULL,
                           output_col_name = NULL,
                           verbose = TRUE,
                           ...) {

  if (verbose) {
    cat("\n", strrep("=", 60), "\n")
    cat(glue("Converting {col_name} to {target_type} \n"))
    cat(strrep("=", 60), "\n")
  }

  # Get conversion and check functions
  converter <- get_type_func(target_type)
  checker <- get_check_func(target_type)

  # Check if already correct type
  if (checker(df[[col_name]])) {
    if (verbose) {
      cat(glue("Column {col_name} is already {target_type}. No conversion needed.\n\n"))
    }
    return(df)
  }

  # Create new column name if not provided
  if (is.null(output_col_name)) {
    output_col_name <- paste0(col_name, "_clean")
  }

  # Apply preprocessing if provided
  if (!is.null(preprocess_fn)) {
    if (verbose) cat("Applying preprocessing...\n")
    df[[output_col_name]] <- preprocess_fn(df[[col_name]])
  }

  # Perform conversion with error handling
  tryCatch({
    if (verbose) cat(glue("Converting {col_name}...\n"))
    df[[output_col_name]] <- converter(df[[output_col_name]], ...)

    if (verbose) {
      cat(glue("{output_col_name} is now: {class(df[[output_col_name]])[1]}\n\n"))
    }
  }, error = function(e) {
    log_error(glue("Failed to convert {output_col_name}: {e$message}"))
    stop(glue("Conversion failed for {output_col_name}: {e$message}"))
  })

  df
}

```

# Fixing Numeric & Price Issues (30–40 minutes)

Focus on the **PRICE** variable (and other numeric variables if needed).

You should investigate:

- **Type issues** (e.g., stored as character instead of numeric),
- **Impossible values** (negative values, weird “huge” values),
- **Sentinel codes** (e.g., `-999`),
- Extra non-numeric characters.

## Suggested tools

- `class()`, `unique()`, `dplyr::count()`, `summary()`
- `readr::parse_number()`
- `as.numeric()`
- `dplyr::mutate()`, `dplyr::case_when()`
- `is.na()`, `sum(is.na(...))`
- `quantile()` to inspect distributions

## Inspect PRICE

> **Tasks:**
>
> 1. Check the **type** of `PRICE` and inspect a sample of values (e.g., using `sample()` or `head()`).

```{r PRICE head}
cat(glue("The starting class (type) of the PRICE column is {class(bids$PRICE)}"), "\n")
print(head(bids$PRICE, 10))
```

> 2. In short sentences, describe what you see:
>    - Is `PRICE` numeric or character?
>    - Do you see obvious anomalies or strange values?

The 'PRICE' column is currently a character vector. There are values that appear to be numeric amounts stored as character strings. No immediate obvious anomalies like negative values or non-numeric characters in the first 10 entries, but further checks revealed there are some prices that the leading 0 is an "O".
## Create a clean numeric version of PRICE

Create a **cleaned numeric version** (e.g., `PRICE_clean`) that:

- Extracts numeric content from strings (if necessary),
- Converts to numeric,
- Keeps track of values that cannot be converted (they will become `NA`).

> **Tasks:**
>
> 1. Use one or more of these:
>    - `readr::parse_number()`
>    - `stringr::str_remove_all()`
>    - `as.numeric()`
>
>    To learn about their use, type `?` before either of them (e.g., `?readr::parse_number()`)

```{r Function: convert to numeric}

# convert column in df to numeric. Handle leading "O" --> 0 if fix_leading_o = TRUE.

#' @param df A data frame
#' @param col_name Name of the column to convert to numeric
#' @param fix_leading_o gsub "O" --> 0
#' @param verbose Whether to print messages
#' @return Data frame with converted column
convert_to_numeric <- function(df, col_name, output_col_name = NULL, fix_leading_o = FALSE, verbose = TRUE) {
  preprocess <- if (fix_leading_o) {
    function(x) {
      problem_rows <- which(is.na(suppressWarnings(as.numeric(x))) & !is.na(x))
      if (verbose && length(problem_rows) > 0) {
        cat(glue("Found {length(problem_rows)} non-numeric value(s), attempting to fix...\n"))
      }
      gsub("^O", "0", x)
    }
  } else {
    NULL
  }

  convert_column(df,
                 col_name = col_name,
                 target_type = "numeric",
                 preprocess_fn = preprocess,
                 output_col_name = output_col_name,
                 verbose = verbose)
}
```

```{r PRICE conversion}
bids <- convert_to_numeric(bids, "PRICE", output_col_name = "PRICE_clean", fix_leading_o = TRUE)
cat("\nFirst 10 PRICE_clean values:\n")
cat(head(bids$PRICE_clean, 10), sep = "\n")
```

> 2. Count how many `NA`s you get in `PRICE_clean`. Explain why some values might be `NA` after parsing.

There are no NA values. Handled leading "O". No other issues that would cause type conversion to fail.

```{r PRICE na count}
# Your code here
na_prices <- sum(is.na(bids$PRICE_clean))
cat(glue("There are {na_prices} NA prices in the PRICE_clean column"))
```


## Define and apply rules for impossible values

Now decide how to handle:

- Negative prices,
- Very large prices that are implausible,
- Sentinel codes (like `-999`).

> **Tasks:**
>
> 1. Use `dplyr::filter()` and summaries (`summary()`, `quantile()`) to identify at least **two kinds** of “bad” price values.


```{r}
# Your code here

summary(bids$PRICE_clean)
quantile(bids$PRICE_clean, probs = seq(0, 1, 0.01), na.rm = TRUE)
bids %>%
  filter(PRICE_clean < 0 | PRICE_clean %in% c(-999, 999, 9999)) %>%
  count(PRICE_clean) %>%
  arrange(desc(n))

```

-999 appears 12 times, this is a placeholder (missing value) and can be treated as NA
There are several entries that are negative, these are invalid because prices can not be negative.


> 2. Write down your **rules** for how to handle each type (e.g., set to `NA`, drop rows, cap values).

Our rules for how to handle each "bad" prices are as follow:
- values like -999, action is to set to NA, since -999 is a     common placeholder for missing or undefined values
- negative values should be set to NA since prices can't be negative and should not be treated as a real price
- implausibly high values (>10) set to na, since based on the data's distribution, prices above 10 are likely errors or outliers.


> 3. Implement those rules in a new variable (e.g., `PRICE_final`) using `dplyr::mutate()` and `dplyr::case_when()`.

```{r}
# Your code here

bids <- bids %>%
  mutate(PRICE_final = case_when(
    PRICE_clean <= 0 ~ NA_real_,
    PRICE_clean > 10 ~ NA_real_,
    TRUE ~ PRICE_clean
  ))


```

> 4. Briefly describe your rules in text and justify them using the **validity** and **completeness** dimensions of data quality.

The following rules were applied to clean the PRICE variable:
Values less than or equal to 0, including values like -999, were set to NA because they are invalid and do not represent realistic bid prices.
Values greater than 10 are also set to NA, as they are so far outside the typical range observed (99th % < 2.1) and are implausible.
All other prices were kept as is.
These rules improve the validity of the data by removing nonsensical/placeholder entries that could bias our analysis. They also enhance completeness by clearly identifying and marking missing or unusable values as NA, rather than leaving them as misleading numerical values.


# Cleaning Categorical & String Fields (30–40 minutes)

Now work on **categorical** and **string** variables, such as:

- `DEVICE_GEO_REGION`
- `DEVICE_GEO_ZIP`
- `RESPONSE_TIME`

Remember: **this dataset only includes bids from Oregon**, so a correct region should be coded as `"OR"`. Other values in `DEVICE_GEO_REGION` represent **modified/mangled versions** of `"OR"`.

Helpful string tools:

- `stringr::str_trim()`, `stringr::str_to_upper()`, `stringr::str_replace()`
- `stringr::str_detect()`, `stringr::str_pad()`, `stringr::str_length()`
- `dplyr::count()` to see category frequencies

## DEVICE_GEO_REGION: standardizing Oregon codes

> **Tasks:**
>
> 1. Use `dplyr::count(DEVICE_GEO_REGION, sort = TRUE)` to list distinct region values and their frequencies.

```{r}
# Your code here

bids %>%
  count(DEVICE_GEO_REGION, sort = TRUE)

```

> 2. Identify **all values** that clearly correspond to **Oregon**, just coded inconsistently (e.g., extra punctuation, different case, misspellings).

In this code I am converting everything to lowercase to make matching easier and using str_detect() to find variants


```{r}
# Your code here

bids %>%
  mutate(region_lower = stringr::str_to_lower(stringr::str_trim(DEVICE_GEO_REGION))) %>%
  count(region_lower, sort = TRUE) %>%
  filter(
    str_detect(region_lower, "^or$")|
    str_detect(region_lower, "oregon") |
    str_detect(region_lower, "xor") |
    str_detect(region_lower, "^or[^a-z]*$")
  )

```

> 3. Define a **standard representation** (`"OR"`) and create a cleaned version (e.g., `DEVICE_GEO_REGION_clean`) where:
>    - All Oregon variants are standardized to `"OR"`.
>    - Other values (if any) are treated according to a rule you decide (e.g., set to `NA` if they cannot be interpreted as Oregon).


```{r}
# Your code here

bids <- bids %>%
  mutate(
    DEVICE_GEO_REGION_clean = case_when(
      str_to_lower(str_trim(DEVICE_GEO_REGION)) %in% c("or", "oregon", "xor") ~ "OR",
      TRUE ~ NA_character_
    )
  )

```

> 4. In **2–3 sentences**, explain:
>    - How you detected inconsistent codes,
>    - The **set-membership rule** you are enforcing (valid region codes for this dataset).

We detected inconsistent region codes by using dplyr::count() to list all distinct values of DEVICE_GEO_REGION and observed multiple variations of "Oregon" such as "Or", "oregon", and "xor". We standardized these using string manipulation functions to detect case differences and common misspellings.
The set-membership rule we enforced is that only region codes clearly representing Oregon, specifically "OR", "oregon", and "xor", are valid. All other values are treated as invalid and set to NA, since this dataset is supposed to contain only bids from Oregon.

## DEVICE_GEO_ZIP: formats and sentinels - Edwin Section

Treat ZIP codes as **strings**, not numbers.

> **Tasks:**
>
> 1. Convert ZIP codes to character and use `dplyr::count()` to inspect common values.

```{r}
# Your code here

```

> 2. Identify suspicious ZIP values:
>    - Too short / too long,
>    - Obvious sentinel codes (e.g., `-999`, `9999`),
>    - Non-digit characters.

```{r}
# Your code here

```

> 3. Decide on a **pattern** for valid ZIP codes (e.g., Oregon 5-digit ZIPs).
>    - Consider using `stringr::str_length()`, `stringr::str_detect()` with a regular expression pattern, and `stringr::str_pad()` if necessary.

```{r}
# Your code here

```

> 4. Create a cleaned version (e.g., `DEVICE_GEO_ZIP_clean`) that:
>    - Trims whitespace,
>    - Converts sentinel codes to `NA`,
>    - Enforces your chosen ZIP pattern.

```{r}
# Your code here

```

> 5. In a short paragraph, describe your rule and why it is reasonable given the context (Oregon-only data).


## RESPONSE_TIME: prefixes and extra characters

The `RESPONSE_TIME` variable may include:

- A **text prefix** (e.g., “RESPONSE_TIME:” or typo’d variants),
- Extra characters at the end (e.g., punctuation),
- Numeric content representing a duration.

> **Tasks:**
>
> 1. Use a small `sample()` of `RESPONSE_TIME` to inspect the raw patterns.

```{r}
n_to_sample <- 10
sampled_data <- bids[sample(1:nrow(bids), n_to_sample, replace = FALSE), c("RESPONSE_TIME")]
print(sampled_data)




```

> 2. Write a rule for how to:
>    - Remove any “RESPONSE_TIME:” prefixes (including likely typos),
>    - Strip extra non-numeric characters from the end,
>    - Convert the remaining content to a numeric type.

  Will exctract all numeric values in the string. These are the response times and will be extracted regardless of the prefix.

> 3. Implement your rule in a cleaned variable (e.g., `RESPONSE_TIME_clean`) using `stringr::str_replace()` or related functions and `as.numeric()`.

```{r Function: RESPONSE_TIME conversion}

#' Convert column to integer with digit extraction
#'
#' @param df A data frame
#' @param col_name Name of column to convert
#' @param extract_digits Whether to extract only digits before converting
#' @param verbose Whether to print messages
#' @return Data frame with converted column
convert_to_integer <- function(df, col_name,
                               extract_digits = FALSE,
                               output_col_name = NULL,
                               verbose = TRUE) {
  preprocess <- if (extract_digits) {
    function(x) {
      if (verbose) cat("Extracting digits from string...\n")
      gsub("[^0-9]", "", x)
    }
  } else {
    NULL
  }

  convert_column(df, col_name, "integer",
                 preprocess_fn = preprocess,
                 output_col_name = output_col_name,
                 verbose = verbose)
}
```

```{r RESPONSE_TIME conversion}
# Your code here
bids <- convert_to_integer(bids, "RESPONSE_TIME", extract_digits = TRUE, output_col_name = "RESPONSE_TIME_clean")
cat("\nFirst 10 RESPONSE_TIME values:\n")
cat(head(bids$RESPONSE_TIME_clean, 10), sep = "\n")
```

> 4. Count how many `NA`s you get in `RESPONSE_TIME_clean` and briefly interpret what those `NA`s mean.

No NA values in response time. Extracted numeric values from string, no other issues caused conversion to fail.

```{r RESPONSE_TIME NA count}
# Your code here
na_response_time <- sum(is.na(bids$RESPONSE_TIME_clean))
cat(glue("There are {na_response_time} NA response times"))
```

> 5. Connect this to the idea of **regular-expression patterns** as a tool for enforcing validity.
If we wanted to check that the prefix was spelled "correctly", we could have used a regular-expression pattern to match "RESPONSE_TIME". We decided to instead use a regex pattern to look for string representations of numeric values. This way we are only extracting the relevent information. This method is not infallible, but for our use-case it appears to work well.



# Date–Time Cleaning: TIMESTAMP (20–30 minutes)

The `TIMESTAMP` field records when a bid occurred, but the **format is inconsistent** (for example, some dates may use `-` and others `/`).

Useful tools:

- `as.character()`
- `stringr::str_detect()`
- `lubridate::parse_date_time()`, `lubridate::ymd_hms()`, `lubridate::mdy_hms()`

## Inspect TIMESTAMP patterns

> **Tasks:**
>
> 1. Look at a small random sample of `TIMESTAMP` values.

```{r}
# Your code here

n_to_sample <- 10
sampled_data <- bids[sample(1:nrow(bids), n_to_sample, replace = FALSE), c("TIMESTAMP")]
print(sampled_data)

num_hyphen_dates <- sum(stringr::str_detect(bids$TIMESTAMP, "-"))
cat(glue("Number of TIMESTAMP values containing '-': {num_hyphen_dates}"), "\n")

num_slash_dates <- sum(stringr::str_detect(bids$TIMESTAMP, "/"))
cat(glue("Number of TIMESTAMP values containing '/': {num_slash_dates}"), "\n")
```

> 2. Use `stringr::str_detect()` to count how many timestamps use `"-"` versus `"/"` in the date.
Number of TIMESTAMP values containing '-': 421614 
Number of TIMESTAMP values containing '/': 1157 
---

Did some exploring of the TIMESTAMP col to identify the issue(s) present. The messy code can be found below.

TL;DR
The code below is messy and inefficient, but it was how I (NEAL) explored the TIMESTAMP and DATE cols. My conclusion, and solution, was to concatenate the date from DATE_UTC and time from TIMESTAMP as strings into an new column and then convert to POSIX.

---
```{r TIMESTAMP exploration}
# Your code here

# EXPLORATORY CODE
# ========================================================================================
# Get DATE_UTC column, which is a character initially, and convert to Date
# Get rows where complete.cases is FALSE

datetime_df <- data.frame(
  DATE_UTC_0 = original_bids$DATE_UTC,
  TIMESTAMP_0 = original_bids$TIMESTAMP
)

datetime_df$DATE <- as.Date(datetime_df$DATE_UTC_0, format = "%Y-%m-%d")

# get TIMESTAMP column, which is a character initially, split each row with delim " "
# Then convert to a dataframe and find all rows where the first column is "NA" <-- string
# Parse the TIMESTAMP column using space delimiter into date and time components
timestamp_split <- strsplit(as.character(datetime_df$TIMESTAMP_0), " ")
timestamp_df <- as.data.frame(do.call(rbind, timestamp_split), stringsAsFactors = FALSE)
colnames(timestamp_df) <- c("TIMESTAMP_date", "TIMESTAMP_time")

# Combine into one df
datetime_df <- cbind(datetime_df, timestamp_df)

DATE_date_NA_indices <- which(is.na(datetime_df$DATE))
TIMESTAMP_date_NA_indices <- which(apply(timestamp_df, 1, function(row) any(row == "NA")))

# Add new column to datetime_dfthat duplicates the TIMESTAMP_date column
# Overwrite TIMESTAMP_date2 with date_col for rows in na_row_indices
datetime_df$TIMESTAMP_date2 <- datetime_df$TIMESTAMP_date
datetime_df$TIMESTAMP_date2[TIMESTAMP_date_NA_indices] <- as.character(datetime_df$DATE[TIMESTAMP_date_NA_indices])

# Check if date_col and TIMESTAMP_date2 are equal (element-wise, after coercion if needed)
are_equal <- all(as.Date(datetime_df$TIMESTAMP_date2) == as.Date(datetime_df$DATE), na.rm = TRUE)
cat("Are date_col and TIMESTAMP_date2 equal?\n")
print(are_equal)
cat(sprintf("Number of mismatches: %d\n", sum(!are_equal, na.rm = TRUE)))


# Create a new TIMESTAMP column that is the concatenation of TIMESTAMP_date2 and TIMESTAMP_time_col
datetime_df$TIMESTAMP_corrected <- paste(datetime_df$TIMESTAMP_date2, datetime_df$TIMESTAMP_time)

# Handle multiple formats in the TIMESTAMP column
format_1 <- "%Y-%m-%d %H:%M:%S"
format_2 <- "%m/%d/%Y %H:%M:%S"

format_1_col <- as.POSIXct(datetime_df$TIMESTAMP_corrected, format = format_1, tz = "UTC")
format_2_col <- as.POSIXct(datetime_df$TIMESTAMP_corrected, format = format_2, tz = "UTC")

not_NA_format_1 <- which(!is.na(format_1_col))
not_NA_format_2 <- which(!is.na(format_2_col))

any_in_both <- any(not_NA_format_1 %in% not_NA_format_2)

combined_indices <- sort(unique(c(not_NA_format_1, not_NA_format_2)))

all_row_indices <- seq_len(nrow(datetime_df))

missing_indices <- setdiff(all_row_indices, combined_indices)

all_corrected_col <- format_1_col
all_corrected_col[not_NA_format_2] <- format_2_col[not_NA_format_2]

datetime_df$TIMESTAMP_posixct <- all_corrected_col

# Check for NA values in TIMESTAMP_posixct column
na_posixct_indices <- which(is.na(datetime_df$TIMESTAMP_posixct))
cat(sprintf("Number of NA values in TIMESTAMP_posixct: %d\n", length(na_posixct_indices)))
```

> 3. Summarize in 1–2 sentences what you observe.
We observed that the TIMESTAMP column had several issues
  1) "NA" in place of a date
  2) multiple date formats mm/dd/yy and yy-mm-dd 

We explored the DATE column and confirmed that all dates available in the TIMESTAMP column (regardless of format) matched the date in DATE. With this information, we decided to concat the date from DATE and time from TIMESTAMP to form a TIMESTAMP_clean column.

## Parse TIMESTAMP into a POSIXct variable

> **Tasks:**
>
> 1. Use `lubridate::parse_date_time()` (or similar) to create `TIMESTAMP_clean`:
>    - Allow for multiple possible formats (e.g., one order for `ymd HMS` and one for `mdy HMS` to see examples scroll paste `?lubridate::parse_date_time()` in your console and scroll all the way to the bottom of the function's help tab),
>    - Specify a time zone (e.g., `"UTC"`).


Our DATE and TIMESTAMP cleaning code was written before we realized that we could use the lubridate package to convert a timestamp with multiple date formats. Our method still results in a uniformly formatted TIMESTAMP_clean

```{r Function: Convert to POSIXct code}

#' Convert column to POSIXct timestamp
#'
#' @param df A data frame
#' @param col_name Name of column to convert
#' @param format Datetime format string
#' @param tz Timezone
#' @param verbose Whether to print messages
#' @return Data frame with converted column
convert_to_posixct <- function(df, col_name,
                               format = "%Y-%m-%d %H:%M:%S",
                               tz = "UTC",
                               verbose = TRUE,
                               date_col = "DATE_UTC",
                               timestamp_col = "TIMESTAMP",
                               output_col = "TIMESTAMP_clean") {

  # checked DATE_UTC and TIMESTAMP columns. DATE_UTC is only the date and TIME STAMP has date and time
  # but some dates are str "NA" and when dates are present, there are two different formats. Compared
  # all dates present in TIMESTAMP to those in DATE_UTC, and found they are the same. Decided to take
  # take the dates from DATE_UTC, which has consistant formatting, and time from TIMESTAMP, and made
  # new column TIMESTAMP, which is the concatenation of the date and time. Renamed old TIMESTAMP
  # column to TIMESTAMP_0.

  # df <- df %>% rename(TIMESTAMP_0 = "TIMESTAMP")

  df <- df %>%
    separate(
      "TIMESTAMP",
      into = c("TIMESTAMP_date", "TIMESTAMP_time"),
      sep = " ",
      remove = FALSE
    ) %>%
    mutate(
      "{output_col}" := paste(.data[[date_col]], .data$TIMESTAMP_time)
    )

  convert_column(df, col_name = output_col, output_col_name = output_col, target_type = "POSIXct",
                 format = format,
                 tz = tz,
                 verbose = verbose,
                 date_col = date_col,
                 timestamp_col = timestamp_col)
}

```


```{r }
# Your code here

bids <- convert_to_posixct(bids, "TIMESTAMP")

```

> 2. Count how many `NA`s result from parsing.

```{r}
# Your code here
TIMESTAMP_na_count <- sum(which(is.na(bids$TIMESTAMP)))
cat(glue("There are {TIMESTAMP_na_count} NAs in the TIMESTAMP column"))

```

> 3. Use `summary()` or `range()` to examine the minimum and maximum of `TIMESTAMP_clean`.

```{r}
# Your code here

summary(bids$TIMESTAMP_clean)
cat("\n")
range(bids$TIMESTAMP_clean)

```

> 4. Briefly comment on:
>    - Whether the time range seems reasonable,
>    - What kinds of entries failed to parse (if any).

A time span of ~8 hours seems reasonable for the data. The range is ("2025-10-21 20:49:05", "2025-10-22 04:32:59"). We do not have any parsing failures or NAs present in the cleaned TIMESTAMP column.

# Keys, Duplicates, and Out-of-Range Values (20–30 minutes)

## Candidate keys and duplicates

Using your earlier reasoning:

> **Tasks:**
>
> 1. Choose one variable or a combination of variables that you **believe should uniquely identify** a row (a key).
> 2. Use `dplyr::count()` to check whether that key is truly unique or if there are duplicates.

```{r}
# Your code here

```

> 3. Check for **full-row duplicates** using `dplyr::distinct()` or by counting occurrences across all columns.

---

NEAL:

Already have code to check for duplicated, but need to exclude the row_id column from the check. Using duplicated() function which should only identify a row as a duplicated if it has already shown up, i.e. row of first occurrence is not listed in duplicate_indices

---
```{r}
# Your code here

#' Find duplicate rows
#'
#' @param df A data frame to check
#' @param exclude_cols Column names to exclude from duplication check
#' @return List with duplicate_indices and num_duplicates
find_duplicates <- function(df, exclude_cols = c("row_id")) {
  check_cols <- setdiff(names(df), exclude_cols)
  duplicate_indices <- which(duplicated(df[, check_cols]))

  list(
    duplicate_indices = duplicate_indices,
    num_duplicates = length(duplicate_indices)
  )
}

duplicate_check <- find_duplicates(bids, exclude_cols = c("row_id"))
duplicate_indices <- duplicate_check$duplicate_indices
num_duplicates <- duplicate_check$num_duplicates

cat(glue("There are {num_duplicates} duplicate rows in the dataset"), "\n")
cat("first 10 duplicate rows: ",duplicate_indices[1:10])
```

> 4. Decide on a rule for handling duplicates:
>    - Drop exact duplicates?
>    - Keep first occurrence and drop the rest?
>    - Something else?
> 5. Implement your rule and create a de-duplicated dataset (e.g., `bids_nodup`).

```{r}
# Your code here

```

> 6. In 2–3 sentences, explain your choice and connect it to the **uniqueness** dimension of data quality.

## Out-of-range latitude/longitude (optional but recommended) - Edwin Section

Look at `DEVICE_GEO_LAT` and `DEVICE_GEO_LONG`.

> **Tasks:**
>
> 1. Use `summary()` or `dplyr::summarise()` to get the minimum and maximum latitude and longitude.

```{r}
# Your code here

```

> 2. Compare the ranges to what you know about **Oregon’s** geographic coordinates (you can approximate based on general US geography; no need for exact boundaries).

```{r}
# Your code here

```

> 3. Decide which coordinates are clearly **implausible**.

```{r}
# Your code here

```

> 4. Define rules to create cleaned versions (e.g., `DEVICE_GEO_LAT_clean`, `DEVICE_GEO_LONG_clean`) where:
>    - Plausible values are kept,
>    - Implausible values are set to `NA` or corresponding rows are removed.

```{r}
# Your code here

```

> 5. Document the rules you used and why.

# Creating a Final Cleaned Dataset (10–15 minutes)

Now pull everything together.

> **Tasks:**
>
> 1. Create a final cleaned dataset (e.g., `bids_clean`) that:
>    - Uses your cleaned variables (`PRICE_final`, `DEVICE_GEO_REGION_clean`, `DEVICE_GEO_ZIP_clean`, `RESPONSE_TIME_clean`, `TIMESTAMP_clean`, cleaned lat/long, etc.),
>    - Drops unnecessary intermediate columns (e.g., `_raw`, helper strings) that you don’t need for analysis.

```{r}
# Your code here

```

> 2. Check the structure of `bids_clean` with `dplyr::glimpse()`.

```{r}
# Your code here

```

> 3. Optionally, write the cleaned dataset to disk (for your own future use) using something like `save()` (see `?save` for its use and an example) to save in `.RData` format **but do not commit the cleaned data file** to the remote repository.

```{r}
# Your code here (optional)

```

> 4. In a short paragraph, describe what your final cleaned dataset now represents and what major issues have been addressed.

# Data Cleaning Log & Reflection (10–15 minutes)

## Data cleaning log (table)

Create a succinct table summarizing the main problems and fixes.

Use the template below and fill it in with your team’s work. **One row (PRICE) is completed as an example; you should complete the remaining rows yourselves.**

| Variable | Problem Detected (brief) | Data Quality Dimension(s) | Cleaning Action (brief) |
|----------|-------------------------|---------------------------|--------------------------|
| PRICE | Includes non-numeric characters | Validity | Extract numeric portion and convert to numeric |
| DEVICE_GEO_REGION | *(team describes)* | *(team selects)* | *(team describes)* |
| DEVICE_GEO_ZIP | *(team describes)* | *(team selects)* | *(team describes)* |
| RESPONSE_TIME | *(team describes)* | *(team selects)* | *(team describes)* |
| TIMESTAMP | *(team describes)* | *(team selects)* | *(team describes)* |
| Other variable(s) | *(optional)* | *(optional)* | *(optional)* |

## Reflection (5–8 sentences)

Write a short reflection addressing:

- Which data cleaning tasks were the most **time-consuming** or **surprising**?
- Which functions or workflows (e.g., `mutate()`, `case_when()`, `str_*()`, `parse_date_time()`, `distinct()`) were most valuable?
- One thing you will do **differently** the next time you receive a new messy dataset.
- Whether/how you used generative AI for this lab and how it affected your understanding.
